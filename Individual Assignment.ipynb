{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text analytics, also known as text mining, is a process of extracting value from large quantities of unstructured text data. While the text itself is structured to make sense to a human being (i.e. A company report split into sensible sections) it is unstructured from an analytics perspective because it doesn’t fit neatly into a relational database or rows and columns of a spreadsheet. Traditionally, the only structured part of text was the name of the document, the date it was created and who created it. Text analytics is particularly useful for information retrieval, pattern recognition, tagging and annotation, information extraction, sentiment assessment and predictive analytics. It could, for example, shed light on what your customers think of your product or service, or highlight the most common issues that your customers complain about.\n",
      "\n",
      "\n",
      "Total number of sentences in this text corpus is 6\n",
      "\n",
      " Text analytics, also known as text mining, is a process of extracting value from large quantities of unstructured text data.\n",
      "\n",
      " While the text itself is structured to make sense to a human being (i.e.\n",
      "\n",
      " A company report split into sensible sections) it is unstructured from an analytics perspective because it doesn’t fit neatly into a relational database or rows and columns of a spreadsheet.\n",
      "\n",
      " Traditionally, the only structured part of text was the name of the document, the date it was created and who created it.\n",
      "\n",
      " Text analytics is particularly useful for information retrieval, pattern recognition, tagging and annotation, information extraction, sentiment assessment and predictive analytics.\n",
      "\n",
      " It could, for example, shed light on what your customers think of your product or service, or highlight the most common issues that your customers complain about.\n"
     ]
    }
   ],
   "source": [
    "# 1.1 sentence tokenisation\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "fileHandler = open('C:/Users/User/Desktop/TXSA Assignment/Individual Assignment Data/Data_1.txt')\n",
    "text = fileHandler.read() # read the file\n",
    "print(text) # print the file\n",
    "\n",
    "sentencetokens = nltk.tokenize.sent_tokenize(text) # perform sentence tokenisation\n",
    "print(\"\\nTotal number of sentences in this text corpus is\",len(sentencetokens))\n",
    "for i in sentencetokens:\n",
    "    print(\"\\n\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'analytics,', 'also', 'known', 'as', 'text', 'mining,', 'is', 'a', 'process', 'of', 'extracting', 'value', 'from', 'large', 'quantities', 'of', 'unstructured', 'text', 'data.', 'While', 'the', 'text', 'itself', 'is', 'structured', 'to', 'make', 'sense', 'to', 'a', 'human', 'being', '(i.e.', 'A', 'company', 'report', 'split', 'into', 'sensible', 'sections)', 'it', 'is', 'unstructured', 'from', 'an', 'analytics', 'perspective', 'because', 'it', 'doesn’t', 'fit', 'neatly', 'into', 'a', 'relational', 'database', 'or', 'rows', 'and', 'columns', 'of', 'a', 'spreadsheet.', 'Traditionally,', 'the', 'only', 'structured', 'part', 'of', 'text', 'was', 'the', 'name', 'of', 'the', 'document,', 'the', 'date', 'it', 'was', 'created', 'and', 'who', 'created', 'it.', 'Text', 'analytics', 'is', 'particularly', 'useful', 'for', 'information', 'retrieval,', 'pattern', 'recognition,', 'tagging', 'and', 'annotation,', 'information', 'extraction,', 'sentiment', 'assessment', 'and', 'predictive', 'analytics.', 'It', 'could,', 'for', 'example,', 'shed', 'light', 'on', 'what', 'your', 'customers', 'think', 'of', 'your', 'product', 'or', 'service,', 'or', 'highlight', 'the', 'most', 'common', 'issues', 'that', 'your', 'customers', 'complain', 'about.\\n']\n"
     ]
    }
   ],
   "source": [
    "#1.2 split function\n",
    "import re\n",
    "tokens = re.split(' ', text) # split by whitespace\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'analytics', ',', 'also', 'known', 'as', 'text', 'mining', ',', 'is', 'a', 'process', 'of', 'extracting', 'value', 'from', 'large', 'quantities', 'of', 'unstructured', 'text', 'data', '.', 'While', 'the', 'text', 'itself', 'is', 'structured', 'to', 'make', 'sense', 'to', 'a', 'human', 'being', '(', 'i.e', '.', 'A', 'company', 'report', 'split', 'into', 'sensible', 'sections', ')', 'it', 'is', 'unstructured', 'from', 'an', 'analytics', 'perspective', 'because', 'it', 'doesn', '’', 't', 'fit', 'neatly', 'into', 'a', 'relational', 'database', 'or', 'rows', 'and', 'columns', 'of', 'a', 'spreadsheet', '.', 'Traditionally', ',', 'the', 'only', 'structured', 'part', 'of', 'text', 'was', 'the', 'name', 'of', 'the', 'document', ',', 'the', 'date', 'it', 'was', 'created', 'and', 'who', 'created', 'it', '.', 'Text', 'analytics', 'is', 'particularly', 'useful', 'for', 'information', 'retrieval', ',', 'pattern', 'recognition', ',', 'tagging', 'and', 'annotation', ',', 'information', 'extraction', ',', 'sentiment', 'assessment', 'and', 'predictive', 'analytics', '.', 'It', 'could', ',', 'for', 'example', ',', 'shed', 'light', 'on', 'what', 'your', 'customers', 'think', 'of', 'your', 'product', 'or', 'service', ',', 'or', 'highlight', 'the', 'most', 'common', 'issues', 'that', 'your', 'customers', 'complain', 'about', '.']\n"
     ]
    }
   ],
   "source": [
    "#1.2 NLTK package\n",
    "import nltk\n",
    "tokens = nltk.tokenize.word_tokenize(text)\n",
    "print([(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'analytic', ',', 'also', 'known', 'a', 'text', 'min', ',', 'i', 'a', 'proces', 'of', 'extract', 'value', 'from', 'large', 'quantit', 'of', 'unstructur', 'text', 'data', '.', 'While', 'the', 'text', 'itself', 'i', 'structur', 'to', 'make', 'sense', 'to', 'a', 'human', 'be', '(', 'i.e', '.', 'A', 'company', 'report', 'split', 'into', 'sens', 'section', ')', 'it', 'i', 'unstructur', 'from', 'an', 'analytic', 'perspect', 'because', 'it', 'doesn', '’', 't', 'fit', 'neat', 'into', 'a', 'relation', 'database', 'or', 'row', 'and', 'column', 'of', 'a', 'spreadsheet', '.', 'Tradition', ',', 'the', 'on', 'structur', 'part', 'of', 'text', 'wa', 'the', 'name', 'of', 'the', 'docu', ',', 'the', 'date', 'it', 'wa', 'creat', 'and', 'who', 'creat', 'it', '.', 'Text', 'analytic', 'i', 'particular', 'use', 'for', 'informa', 'retriev', ',', 'pattern', 'recogni', ',', 'tagg', 'and', 'annota', ',', 'informa', 'extrac', ',', 'senti', 'assess', 'and', 'predict', 'analytic', '.', 'It', 'could', ',', 'for', 'example', ',', 'sh', 'light', 'on', 'what', 'your', 'customer', 'think', 'of', 'your', 'product', 'or', 'service', ',', 'or', 'highlight', 'the', 'most', 'common', 'issu', 'that', 'your', 'customer', 'complain', 'about', '.']\n"
     ]
    }
   ],
   "source": [
    "#2.2 RE word stemming\n",
    "import re\n",
    "import nltk\n",
    "def stem(word):\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment|ible|al|ally|ful|ion|tion)?$' # define suffixes\n",
    "    stem, suffix = re.findall(regexp,word)[0]\n",
    "    return stem\n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(text)\n",
    "print([stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'analyt', ',', 'also', 'known', 'as', 'text', 'mine', ',', 'is', 'a', 'process', 'of', 'extract', 'valu', 'from', 'larg', 'quantiti', 'of', 'unstructur', 'text', 'data', '.', 'while', 'the', 'text', 'itself', 'is', 'structur', 'to', 'make', 'sens', 'to', 'a', 'human', 'be', '(', 'i.e', '.', 'A', 'compani', 'report', 'split', 'into', 'sensibl', 'section', ')', 'it', 'is', 'unstructur', 'from', 'an', 'analyt', 'perspect', 'becaus', 'it', 'doesn', '’', 't', 'fit', 'neatli', 'into', 'a', 'relat', 'databas', 'or', 'row', 'and', 'column', 'of', 'a', 'spreadsheet', '.', 'tradit', ',', 'the', 'onli', 'structur', 'part', 'of', 'text', 'wa', 'the', 'name', 'of', 'the', 'document', ',', 'the', 'date', 'it', 'wa', 'creat', 'and', 'who', 'creat', 'it', '.', 'text', 'analyt', 'is', 'particularli', 'use', 'for', 'inform', 'retriev', ',', 'pattern', 'recognit', ',', 'tag', 'and', 'annot', ',', 'inform', 'extract', ',', 'sentiment', 'assess', 'and', 'predict', 'analyt', '.', 'It', 'could', ',', 'for', 'exampl', ',', 'shed', 'light', 'on', 'what', 'your', 'custom', 'think', 'of', 'your', 'product', 'or', 'servic', ',', 'or', 'highlight', 'the', 'most', 'common', 'issu', 'that', 'your', 'custom', 'complain', 'about', '.']\n"
     ]
    }
   ],
   "source": [
    "#2.2 Porter Stemmer word stemming\n",
    "import nltk\n",
    "tokens = nltk.tokenize.word_tokenize(text)\n",
    "porter = nltk.PorterStemmer()\n",
    "print([porter.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'analys', ',', 'also', 'known', 'as', 'text', 'min', ',', 'is', 'a', 'process', 'of', 'extract', 'valu', 'from', 'larg', 'quant', 'of', 'unstruct', 'text', 'dat', '.', 'whil', 'the', 'text', 'itself', 'is', 'structured', 'to', 'mak', 'sens', 'to', 'a', 'hum', 'being', '(', 'i.e', '.', 'a', 'company', 'report', 'split', 'into', 'sens', 'sect', ')', 'it', 'is', 'unstruct', 'from', 'an', 'analys', 'perspect', 'becaus', 'it', 'doesn', '’', 't', 'fit', 'neat', 'into', 'a', 'rel', 'databas', 'or', 'row', 'and', 'column', 'of', 'a', 'spreadsheet', '.', 'tradit', ',', 'the', 'on', 'structured', 'part', 'of', 'text', 'was', 'the', 'nam', 'of', 'the', 'docu', ',', 'the', 'dat', 'it', 'was', 'cre', 'and', 'who', 'cre', 'it', '.', 'text', 'analys', 'is', 'particul', 'us', 'for', 'inform', 'retriev', ',', 'pattern', 'recognit', ',', 'tag', 'and', 'annot', ',', 'inform', 'extract', ',', 'senty', 'assess', 'and', 'predict', 'analys', '.', 'it', 'could', ',', 'for', 'exampl', ',', 'shed', 'light', 'on', 'what', 'yo', 'custom', 'think', 'of', 'yo', 'produc', 'or', 'serv', ',', 'or', 'highlight', 'the', 'most', 'common', 'issu', 'that', 'yo', 'custom', 'complain', 'about', '.']\n"
     ]
    }
   ],
   "source": [
    "#2.2 Lancaster Stemmer word stemming\n",
    "import nltk\n",
    "tokens = nltk.tokenize.word_tokenize(text)\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the text corpus is 154\n",
      "Total number of words filtered after removing stop words and punctuation is 73\n",
      "\n",
      " ['text', 'analytics', 'also', 'known', 'text', 'mining', 'process', 'extracting', 'value', 'large', 'quantities', 'unstructured', 'text', 'data', 'text', 'structured', 'make', 'sense', 'human', 'i.e', 'company', 'report', 'split', 'sensible', 'sections', 'unstructured', 'analytics', 'perspective', 'fit', 'neatly', 'relational', 'database', 'rows', 'columns', 'spreadsheet', 'traditionally', 'structured', 'part', 'text', 'name', 'document', 'date', 'created', 'created', 'text', 'analytics', 'particularly', 'useful', 'information', 'retrieval', 'pattern', 'recognition', 'tagging', 'annotation', 'information', 'extraction', 'sentiment', 'assessment', 'predictive', 'analytics', 'could', 'example', 'shed', 'light', 'customers', 'think', 'product', 'service', 'highlight', 'common', 'issues', 'customers', 'complain']\n",
      "\n",
      " ['as', 'is', 'a', 'of', 'from', 'of', 'while', 'the', 'itself', 'is', 'to', 'to', 'a', 'being', 'a', 'into', 'it', 'is', 'from', 'an', 'because', 'it', 'doesn', 't', 'into', 'a', 'or', 'and', 'of', 'a', 'the', 'only', 'of', 'was', 'the', 'of', 'the', 'the', 'it', 'was', 'and', 'who', 'it', 'is', 'for', 'and', 'and', 'it', 'for', 'on', 'what', 'your', 'of', 'your', 'or', 'or', 'the', 'most', 'that', 'your', 'about']\n",
      "\n",
      "Total number of stops words filtered is 61\n"
     ]
    }
   ],
   "source": [
    "#3.1 filter stop words and punctuation\n",
    "import nltk, string\n",
    "tokens = nltk.tokenize.word_tokenize(text.lower())\n",
    "print(\"Total number of words in the text corpus is\",len(tokens))\n",
    "\n",
    "stopTokens = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation+'’')\n",
    "stopTokens2 = nltk.corpus.stopwords.words(\"english\")\n",
    "filteredTokens = [] # to store filtered words\n",
    "unwantedTokens = [] # to store stop words found\n",
    "\n",
    "for t in tokens:\n",
    "    if t not in stopTokens:\n",
    "        filteredTokens.append(t)\n",
    "    if t in stopTokens2:\n",
    "        unwantedTokens.append(t)\n",
    "\n",
    "print(\"Total number of words filtered after removing stop words and punctuation is\",len(filteredTokens))\n",
    "print('\\n',filteredTokens)\n",
    "print('\\n',unwantedTokens)\n",
    "print(\"\\nTotal number of stops words filtered is\",len(unwantedTokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP is important for scientific, economic, social, and cultural reasons. NLP is experiencing rapid growth as its theories and methods are deployed in a variety of new language technologies. \n",
      "\n",
      "Total number of sentences in this text corpus is 2\n",
      "\n",
      " NLP is important for scientific, economic, social, and cultural reasons.\n",
      "\n",
      " NLP is experiencing rapid growth as its theories and methods are deployed in a variety of new language technologies.\n",
      " \n",
      "Total number of words in the text corpus is 34\n",
      "Total number of words filtered after removing punctuation is 29\n",
      "\n",
      " ['NLP', 'is', 'important', 'for', 'scientific', 'economic', 'social', 'and', 'cultural', 'reasons', 'NLP', 'is', 'experiencing', 'rapid', 'growth', 'as', 'its', 'theories', 'and', 'methods', 'are', 'deployed', 'in', 'a', 'variety', 'of', 'new', 'language', 'technologies']\n",
      "\n",
      " [('NLP', 'NNP'), ('is', 'VBZ'), ('important', 'JJ'), ('for', 'IN'), ('scientific', 'JJ'), ('economic', 'JJ'), ('social', 'JJ'), ('and', 'CC'), ('cultural', 'JJ'), ('reasons', 'NNS'), ('NLP', 'NNP'), ('is', 'VBZ'), ('experiencing', 'VBG'), ('rapid', 'JJ'), ('growth', 'NN'), ('as', 'IN'), ('its', 'PRP$'), ('theories', 'NNS'), ('and', 'CC'), ('methods', 'NNS'), ('are', 'VBP'), ('deployed', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('variety', 'NN'), ('of', 'IN'), ('new', 'JJ'), ('language', 'NN'), ('technologies', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "import nltk,string\n",
    "fileHandler = open('C:/Users/User/Desktop/TXSA Assignment/Individual Assignment Data/Data_2.txt')\n",
    "text2 = fileHandler.read()\n",
    "print(text2)\n",
    "\n",
    "sentencetokens = nltk.tokenize.sent_tokenize(text2)\n",
    "print(\"\\nTotal number of sentences in this text corpus is\",len(sentencetokens))\n",
    "for i in sentencetokens:\n",
    "    print(\"\\n\",i)\n",
    "print(\" \")\n",
    "\n",
    "#4.1 POS tagging using NLTK POS tagger\n",
    "\n",
    "tokens2 = nltk.tokenize.word_tokenize(text2)\n",
    "\n",
    "print(\"Total number of words in the text corpus is\",len(tokens2))\n",
    "\n",
    "stopTokens = list(string.punctuation)\n",
    "filteredTokens2 = []\n",
    "\n",
    "for t in tokens2:\n",
    "    if t not in stopTokens:\n",
    "        filteredTokens2.append(t)\n",
    "\n",
    "print(\"Total number of words filtered after removing punctuation is\",len(filteredTokens2))\n",
    "print('\\n',filteredTokens2)\n",
    "print('\\n',nltk.pos_tag(filteredTokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLP', 'NNP'), ('is', 'VBZ'), ('important', 'JJ'), ('for', 'IN'), ('scientific', 'JJ'), ('economic', 'JJ'), ('social', 'JJ'), ('and', 'CC'), ('cultural', 'JJ'), ('reasons', 'NNS'), ('NLP', 'NNP'), ('is', 'VBZ'), ('experiencing', 'VBG'), ('rapid', 'JJ'), ('growth', 'NN'), ('as', 'IN'), ('its', 'PRP$'), ('theories', 'NNS'), ('and', 'CC'), ('methods', 'NNS'), ('are', 'VBP'), ('deployed', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('variety', 'NN'), ('of', 'IN'), ('new', 'JJ'), ('language', 'NN'), ('technologies', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "#4.1 textblob POS tagger\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "tokens2 = TextBlob(text2)\n",
    "print(tokens2.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLP', 'NN'), ('is', 'NNS'), ('important', 'NN'), ('for', 'NN'), ('scientific', 'NN'), ('economic', 'NN'), ('social', 'NN'), ('and', 'NN'), ('cultural', 'NN'), ('reasons', 'NNS'), ('NLP', 'NN'), ('is', 'NNS'), ('experiencing', 'VBG'), ('rapid', 'NN'), ('growth', 'NN'), ('as', 'NNS'), ('its', 'NNS'), ('theories', 'VBZ'), ('and', 'NN'), ('methods', 'NNS'), ('are', 'NN'), ('deployed', 'VBD'), ('in', 'NN'), ('a', 'NN'), ('variety', 'NN'), ('of', 'NN'), ('new', 'NN'), ('language', 'NN'), ('technologies', 'VBZ')]\n"
     ]
    }
   ],
   "source": [
    "#4.1 RE tagger\n",
    "\n",
    "patterns = [\n",
    "     (r'.*ing$', 'VBG'),               # gerunds\n",
    "     (r'.*ed$', 'VBD'),                # simple past\n",
    "     (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "     (r'.*ould$', 'MD'),               # modals\n",
    "     (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "     (r'.*s$', 'NNS'),                 # plural nouns\n",
    "     (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "     (r'.*', 'NN'),                    # nouns (default)\n",
    "     (r'^\\d+$', 'CD'),\n",
    "     (r'.*ing$', 'VBG'),               # gerunds, i.e. wondering\n",
    "     (r'.*ment$', 'NN'),               # i.e. wonderment\n",
    "     (r'.*ful$', 'JJ')                 # i.e. wonderful\n",
    " ]\n",
    "\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "tagger=nltk.tag.sequential.RegexpTagger(patterns)\n",
    "print(tagger.tag(filteredTokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
